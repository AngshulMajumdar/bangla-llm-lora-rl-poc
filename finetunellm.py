# -*- coding: utf-8 -*-
"""FineTuneLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IegEe5ii-LyAYJhEQ0HAoPtfEOjnq7eS
"""

!pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece
!pip uninstall -y bitsandbytes
!pip install -U bitsandbytes

import os
import shutil
import subprocess

# Try to unmount Google Drive if already mounted
subprocess.run(["fusermount", "-u", "/content/drive"], stderr=subprocess.DEVNULL)

# Remove any leftover mount directories
shutil.rmtree("/content/drive", ignore_errors=True)
os.makedirs("/content/drive", exist_ok=True)

# Fresh mount
from google.colab import drive
drive.mount("/content/drive", force_remount=True)

import os

BASE_DIR = "/content/drive/MyDrive/bangla_llm"
DATA_DIR = os.path.join(BASE_DIR, "data")
CKPT_DIR = os.path.join(BASE_DIR, "checkpoints")
FINAL_DIR = os.path.join(BASE_DIR, "final_model")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CKPT_DIR, exist_ok=True)
os.makedirs(FINAL_DIR, exist_ok=True)

print("Project directories ready:")
print(DATA_DIR)
print(CKPT_DIR)
print(FINAL_DIR)

from transformers import AutoTokenizer

MODEL_NAME = "Qwen/Qwen2.5-0.5B"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    use_fast=True,
    trust_remote_code=True
)

print("Tokenizer loaded")

from datasets import load_dataset

DATA_FILE = "/content/drive/MyDrive/bangla_llm/data/train.txt"

dataset = load_dataset("text", data_files={"train": DATA_FILE})

def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        max_length=512,
        padding=False
    )

tokenized_ds = dataset.map(
    tokenize,
    batched=True,
    remove_columns=["text"]
)

print("Defined variable: tokenized_ds")
print(tokenized_ds)

"""from datasets import load_dataset

DATA_FILE = "/content/drive/MyDrive/bangla_llm/data/train.txt"

dataset = load_dataset("text", data_files={"train": DATA_FILE})

def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        max_length=512,
        padding=False
    )

tokenized_ds = dataset.map(
    tokenize,
    batched=True,
    remove_columns=["text"]
)

print("Tokenization successful")
print(tokenized_ds)
print("Example tokenized item:", tokenized_ds["train"][0])

"""

import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

MODEL_NAME = "Qwen/Qwen2.5-0.5B"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

print("Model loaded in fp16 (no bitsandbytes)")

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

import inspect
from transformers import TrainingArguments

CKPT_DIR = "/content/drive/MyDrive/bangla_llm/checkpoints"

sig = inspect.signature(TrainingArguments.__init__)
params = sig.parameters

args = dict(
    output_dir=CKPT_DIR,
    overwrite_output_dir=False,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    max_steps=1250,            # <<< THIS IS THE CHANGE (was implicit 12500)
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=50,
    save_total_limit=3,
    report_to="none",
    optim="paged_adamw_8bit",
)


# Handle version differences: evaluation_strategy vs eval_strategy
if "evaluation_strategy" in params:
    args["evaluation_strategy"] = "no"
elif "eval_strategy" in params:
    args["eval_strategy"] = "no"

training_args = TrainingArguments(**args)
print("TrainingArguments ready (compatible).")

import os

def get_latest_checkpoint(path):
    if not os.path.exists(path):
        return None
    ckpts = [
        os.path.join(path, d)
        for d in os.listdir(path)
        if d.startswith("checkpoint-")
    ]
    return max(ckpts, key=os.path.getmtime) if ckpts else None

resume_ckpt = get_latest_checkpoint(CKPT_DIR)
print("Resuming from:", resume_ckpt)

from transformers import Trainer, DataCollatorForLanguageModeling

collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],  # <-- THIS VARIABLE NOW EXISTS
    tokenizer=tokenizer,
    data_collator=collator,
)

trainer.train(resume_from_checkpoint=resume_ckpt)

import os

ROOT = "/content/drive/MyDrive"

found = []

for root, dirs, files in os.walk(ROOT):
    for d in dirs:
        if d.startswith("checkpoint-"):
            found.append(os.path.join(root, d))

print("Found checkpoints:")
for p in found:
    print(p)

print("\nTotal:", len(found))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

MODEL_NAME = "Qwen/Qwen2.5-0.5B"
CKPT_PATH = "/content/drive/MyDrive/bangla_llm/checkpoints/checkpoint-4001"
FINAL_DIR = "/content/drive/MyDrive/bangla_llm/final_model"

# Load base model (fp16, no bitsandbytes)
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# Attach LoRA adapter from checkpoint
model = PeftModel.from_pretrained(base_model, CKPT_PATH)
model.eval()

# Save clean final model
model.save_pretrained(FINAL_DIR)

# Save tokenizer as well
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    use_fast=True,
    trust_remote_code=True
)
tokenizer.save_pretrained(FINAL_DIR)

print("Final PoC model saved to:", FINAL_DIR)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

FINAL_DIR = "/content/drive/MyDrive/bangla_llm/final_model"
MODEL_NAME = "Qwen/Qwen2.5-0.5B"

tokenizer = AutoTokenizer.from_pretrained(FINAL_DIR)

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

model = PeftModel.from_pretrained(base_model, FINAL_DIR)
model.eval()

print("PoC model reloaded successfully")

prompt = "বাংলা ভাষা প্রযুক্তির ভবিষ্যৎ সম্পর্কে একটি সংক্ষিপ্ত অনুচ্ছেদ লিখুন।"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    output = model.generate(
        **inputs,
        max_new_tokens=120,
        do_sample=True,
        temperature=0.8,
        top_p=0.9
    )

print(tokenizer.decode(output[0], skip_special_tokens=True))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

MODEL_NAME = "Qwen/Qwen2.5-0.5B"
FINAL_DIR = "/content/drive/MyDrive/bangla_llm/final_model"  # LoRA adapter dir

# 1️⃣ Tokenizer ALWAYS comes from base model
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)

# 2️⃣ Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# 3️⃣ Attach LoRA adapter
model = PeftModel.from_pretrained(
    base_model,
    FINAL_DIR
)

# 4️⃣ TRAIN MODE for RL (important)
model.train()

print("✅ Policy model loaded correctly (base + LoRA, training mode)")
print(tokenizer.name_or_path)

import torch

# Force trainable LoRA adapter
try:
    model.enable_adapter_layers()
except Exception:
    pass

# Some PEFT versions need this:
if hasattr(model, "set_adapter"):
    # keep default adapter active if present
    try:
        model.set_adapter(model.active_adapter)
    except Exception:
        pass

# Mark only LoRA params trainable (robust)
trainable = 0
total = 0
for name, p in model.named_parameters():
    total += 1
    if "lora_" in name or "lora" in name:
        p.requires_grad = True
        trainable += 1
    else:
        p.requires_grad = False

print(f"Trainable params tensors: {trainable} / {total}")

# Sanity: if still zero, you did NOT load a LoRA adapter correctly
assert trainable > 0, "No LoRA parameters found/trainable. You loaded the wrong adapter directory."

optimizer = torch.optim.AdamW(
    [p for p in model.parameters() if p.requires_grad],
    lr=1e-5
)

print("✅ Optimizer created on LoRA params only.")

import torch
import re

# Make sure we can pad if needed
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False  # safer for training

def bangla_reward(text):
    bangla = len(re.findall(r'[\u0980-\u09FF]', text))
    english = sum(c.isascii() and c.isalpha() for c in text)
    total = max(len(text), 1)
    return (bangla - english) / total

prompts = [
    "বাংলা ভাষার ভবিষ্যৎ কী?",
    "কৃত্রিম বুদ্ধিমত্তা কীভাবে সমাজ বদলাচ্ছে?",
    "ভারতে ভাষা প্রযুক্তির গুরুত্ব কী?"
]

model.train()

for step in range(20):
    prompt = prompts[step % len(prompts)]
    inp = tokenizer(prompt, return_tensors="pt").to(model.device)
    prompt_len = inp["input_ids"].shape[1]

    # 1) Sample completion (no gradients here, that's fine)
    with torch.no_grad():
        full_ids = model.generate(
            **inp,
            max_new_tokens=80,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
        )

    gen_ids = full_ids[:, prompt_len:]
    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)

    r = bangla_reward(text)
    r_t = torch.tensor(r, device=model.device, dtype=torch.float32)

    # 2) Compute logprob(sample) WITH gradients using a forward pass
    out = model(full_ids)
    logits = out.logits  # [1, full_len, vocab]

    logits_for_gen = logits[:, prompt_len-1:-1, :]      # length = gen_len
    targets_gen    = full_ids[:, prompt_len:]           # length = gen_len

    log_probs = torch.log_softmax(logits_for_gen, dim=-1)
    selected = log_probs.gather(-1, targets_gen.unsqueeze(-1)).squeeze(-1)
    logprob_sum = selected.sum()

    loss = -r_t * logprob_sum

    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

    print(f"[RL step {step}] reward={r:.3f} loss={loss.item():.4f} | sample: {text[:120]}")